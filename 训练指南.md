# 请先看readme，这个指南不是必须的
以此为例  
第一次运行model.py  
model = PPO("MultiInputPolicy",  
                env=env,  
                policy_kwargs=policy_kwargs, verbose=1, seed=512,  
                n_steps=4096,# 这个表示每online收集4096步的数据开始训练  
                batch_size=128,# 这个表示每一次更新选择时一次梯度下降的数据，一般学习率调整多少倍，这个也调整多少倍  
                tensorboard_log=r"logs\zote08",  
                learning_rate=7.5e-5, # 学习率，先找到合适的学习率，再和batch_size同时调整倍数  
                n_epochs=8,# 8表示数据重用8次，随训练进度而减小  
                gamma=0.85, gae_lambda=0.95,  
                clip_range=0.2,  
                ent_coef=0.01, # 表示小概率发生的事件具有更大的信息量，比如“明天太阳一定会升起”，这句话信息量为0  
                vf_coef=0.5, # 表示价值函数调整时幅度会再乘0.5  
                # max_grad_norm=0.5  
                )  
model.py里CheckpointCallback的save_freq表示定期保存的步数,建议比上面的n_steps加一，这样每次算完都会及时保存

（这一行的理解不一定对）tensorboard里，比如说有一个点的表现特别好，处于峰值，
读出来是10089步，保存的模型最接近的是比这个step大一点点的0k_add__10090_steps.zip，说明保存的是根据这此的数据梯度下降后的模型，
然后之前那个点是峰值，说明反向优化了，所以，最好的模型应该是0k_add__10090_steps.zip紧挨着的之前那个，
也就是说，长n_steps本身也起到了评估的作用，因为模型只在梯度下降后才改变

-----------------------------------------------
第二次及以后运行continue.py

先修改PPO.load后面的路径

调整n_steps的话别忘了去model.py里调整CheckpointCallback的save_freq

model.learn的reset_num_timesteps=False表示tensorboard里会接着往下画，而非另一条线

----------------------------------------------

强化学习有不稳定的特点，并非训练都能稳步提升，反向优化是正常现象

正确评估模型是非常高成本的，就是让模型打10局，出来的成绩也大概率相差比较大，然而如果每一个模型都打100局的话，时间成本又过于高昂

--------------------------------------------
运行test.py评估模型，evaluate_policy里的n_eval_episodes参数表示每个模型评估所用的局数

因为有相关wrapper，小骑士每扣1滴血或者同时扣2滴就是1局，90表示10局（如果每次只扣1滴血的话）